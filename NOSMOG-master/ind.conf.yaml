global:
  num_layers: 2
  hidden_dim: 128

# ============================================================
cora:
  SAGE:
    fan_out: 5,5
    learning_rate: 0.01
    dropout_ratio: 0
    weight_decay: 0.0005
  
  GCN:
    hidden_dim: 64
    dropout_ratio: 0.8
    weight_decay: 0.001
  
  MLP:
    learning_rate: 0.01
    weight_decay: 0.005
    dropout_ratio: 0.6
    dw_walk_length: 50
    dw_num_walks: 1
    dw_window_size: 3
    dw_iter: 3
    dw_emb_size: 64
    adv_eps: 0.05
    feat_distill_weight: 0.1

  GAT:
    dropout_ratio: 0.6
    weight_decay: 0.01
    num_heads: 8
    attn_dropout_ratio: 0.3

  APPNP:
    dropout_ratio: 0.5
    weight_decay: 0.01

# ============================================================
citeseer:
  SAGE:
    fan_out: 5,5
    learning_rate: 0.01
    dropout_ratio: 0
    weight_decay: 0.0005

  GCN:
    hidden_dim: 64
    dropout_ratio: 0.8
    weight_decay: 0.001
  
  MLP:
    learning_rate: 0.01
    weight_decay: 0.001
    dropout_ratio: 0.1
    dw_walk_length: 50
    dw_num_walks: 3
    dw_window_size: 5
    dw_iter: 3
    dw_emb_size: 64
    adv_eps: 0.05
    feat_distill_weight: 0.3

  GAT:
    dropout_ratio: 0.6
    weight_decay: 0.01
    num_heads: 8
    attn_dropout_ratio: 0.3

  APPNP:
    dropout_ratio: 0.5
    weight_decay: 0.01

# ============================================================
pubmed:
  SAGE:
    fan_out: 5,5
    learning_rate: 0.01
    dropout_ratio: 0
    weight_decay: 0.0005

  GCN:
    hidden_dim: 64
    dropout_ratio: 0.8
    weight_decay: 0.001
  
  MLP:
    learning_rate: 0.005
    weight_decay: 0.001
    dropout_ratio: 0.4
    dw_walk_length: 50
    dw_num_walks: 3
    dw_window_size: 5
    dw_iter: 1
    dw_emb_size: 16
    adv_eps: 0.001
    feat_distill_weight: 0.9

  GAT:
    dropout_ratio: 0.6
    weight_decay: 0.01
    num_heads: 8
    attn_dropout_ratio: 0.3

  APPNP:
    dropout_ratio: 0.5
    weight_decay: 0.01

# ============================================================
a-computer:
  SAGE:
    fan_out: 5,5
    learning_rate: 0.01
    dropout_ratio: 0
    weight_decay: 0.0005

  GCN:
    hidden_dim: 64
    dropout_ratio: 0.8
    weight_decay: 0.001
  
  MLP:
    learning_rate: 0.003
    weight_decay: 0.005
    dropout_ratio: 0.1
    dw_walk_length: 70
    dw_num_walks: 5
    dw_window_size: 5
    dw_iter: 3
    dw_emb_size: 64
    adv_eps: 0.06
    feat_distill_weight: 1e-09

  GAT:
    dropout_ratio: 0.6
    weight_decay: 0.01
    num_heads: 8
    attn_dropout_ratio: 0.3
  
  APPNP:
    dropout_ratio: 0.5
    weight_decay: 0.01

# ============================================================
a-photo:
  SAGE:
    fan_out: 5,5
    learning_rate: 0.01
    dropout_ratio: 0
    weight_decay: 0.0005

  GCN:
    hidden_dim: 64
    dropout_ratio: 0.8
    weight_decay: 0.001
  
  MLP:
    learning_rate: 0.001
    weight_decay: 0.001
    dropout_ratio: 0.1
    dw_walk_length: 50
    dw_num_walks: 1
    dw_window_size: 3
    dw_iter: 1
    dw_emb_size: 32
    adv_eps: 0.1
    feat_distill_weight: 1e-08

  GAT:
    dropout_ratio: 0.6
    weight_decay: 0.01
    num_heads: 8
    attn_dropout_ratio: 0.3

  APPNP:
    dropout_ratio: 0.5
    weight_decay: 0.01

# ============================================================
ogbn-arxiv:
  MLP:
    learning_rate: 0.01
    num_layers: 3
    hidden_dim: 256
    weight_decay: 0
    dropout_ratio: 0.2
    norm_type: batch
    dw_walk_length: 50
    dw_num_walks: 5
    dw_window_size: 5
    dw_iter: 1
    dw_emb_size: 128
    adv_eps: 0.045
    feat_distill_weight: 1e-6

  MLP3w4:
    num_layers: 3
    hidden_dim: 1024
    weight_decay: 0
    dropout_ratio: 0.5
    norm_type: batch

  SAGE:
    num_layers: 3
    hidden_dim: 256
    dropout_ratio: 0.2
    learning_rate: 0.01
    weight_decay: 0
    norm_type: batch
    fan_out: 5,10,15


# ============================================================
ogbn-products:
  MLP:
    learning_rate: 0.003
    num_layers: 3
    hidden_dim: 256
    dropout_ratio: 0.5
    weight_decay: 0
    norm_type: batch
    batch_size: 1024
    dw_walk_length: 50
    dw_num_walks: 5
    dw_window_size: 5
    dw_iter: 1
    dw_emb_size: 64
    adv_eps: 0.005
    feat_distill_weight: 1e-7

  MLP3w8:
    num_layers: 3
    hidden_dim: 2048
    dropout_ratio: 0.2
    learning_rate: 0.01
    weight_decay: 0
    norm_type: batch
    batch_size: 4096

  SAGE:
    num_layers: 2
    hidden_dim: 256
    dropout_ratio: 0.5
    learning_rate: 0.003
    weight_decay: 0
    norm_type: batch
    fan_out: 5,10
    batch_size: 4096
